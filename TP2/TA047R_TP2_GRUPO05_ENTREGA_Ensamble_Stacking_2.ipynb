{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383458ff-4f5b-4724-9b63-1646576824b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "#Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "\n",
    "#Métricas\n",
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer\n",
    "\n",
    "#Configuración Warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "#Regressors\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "\n",
    "#Análisis de Sentimientos\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36689fed",
   "metadata": {
    "id": "36689fed",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_stories_train = pd.read_csv('data/train.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Wsdp631-9TUx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wsdp631-9TUx",
    "outputId": "0430dfa1-5221-4f50-bcbc-17a1bfd536ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>project</th>\n",
       "      <th>storypoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5660</td>\n",
       "      <td>Error enabling Appcelerator services during ap...</td>\n",
       "      <td>When creating the default app, I encountered t...</td>\n",
       "      <td>project8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9014</td>\n",
       "      <td>Create a maintenance branch</td>\n",
       "      <td>As a developer, I'd like to have a maintenance...</td>\n",
       "      <td>project6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4094</td>\n",
       "      <td>Service Activity Monitoring Backend integrated...</td>\n",
       "      <td>SAM API used by SAM GUI</td>\n",
       "      <td>project1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>811</td>\n",
       "      <td>fs::enter(rootfs) does not work if 'rootfs' is...</td>\n",
       "      <td>I noticed this when I was testing the unified ...</td>\n",
       "      <td>project5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4459</td>\n",
       "      <td>transform processor with script option is broken</td>\n",
       "      <td>Creating the following stream throws exception...</td>\n",
       "      <td>project6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              title  \\\n",
       "0  5660  Error enabling Appcelerator services during ap...   \n",
       "1  9014                        Create a maintenance branch   \n",
       "2  4094  Service Activity Monitoring Backend integrated...   \n",
       "3   811  fs::enter(rootfs) does not work if 'rootfs' is...   \n",
       "4  4459   transform processor with script option is broken   \n",
       "\n",
       "                                         description   project  storypoint  \n",
       "0  When creating the default app, I encountered t...  project8           3  \n",
       "1  As a developer, I'd like to have a maintenance...  project6           5  \n",
       "2                            SAM API used by SAM GUI  project1           5  \n",
       "3  I noticed this when I was testing the unified ...  project5           2  \n",
       "4  Creating the following stream throws exception...  project6           2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_stories_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ebf362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              int64\n",
       "title          object\n",
       "description    object\n",
       "project        object\n",
       "storypoint      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_stories_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "agVdguy6-pvx",
   "metadata": {
    "id": "agVdguy6-pvx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de columnas: 5\n",
      "Cantidad de filas: 7900\n"
     ]
    }
   ],
   "source": [
    "# Vemos cantidad de filas y columnas\n",
    "user_stories_train_size = user_stories_train.shape\n",
    "print(\"Cantidad de columnas: %d\" % user_stories_train_size[1])\n",
    "print(\"Cantidad de filas: %d\" % user_stories_train_size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9T8yqcOQFTi2",
   "metadata": {
    "id": "9T8yqcOQFTi2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se eliminaron: 0 filas duplicadas\n"
     ]
    }
   ],
   "source": [
    "# Analizamos si hay filas duplicadas\n",
    "user_stories_train_total = len(user_stories_train)\n",
    "user_stories_train_unique = user_stories_train.drop_duplicates()\n",
    "user_stories_train_unique_size = len(user_stories_train_unique)\n",
    "print(f'Se eliminaron: {user_stories_train_total - user_stories_train_unique_size} filas duplicadas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "zt0V57ELFrU-",
   "metadata": {
    "id": "zt0V57ELFrU-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "title          0\n",
       "description    0\n",
       "project        0\n",
       "storypoint     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos cantidad de datos faltantes\n",
    "user_stories_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "n9MjyN8iFw1V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9MjyN8iFw1V",
    "outputId": "e6d0b23b-3376-4b35-b499-b250f89ef493"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "storypoint\n",
       "3     23.392405\n",
       "5     21.430380\n",
       "1     20.620253\n",
       "2     16.151899\n",
       "8     12.974684\n",
       "4      2.101266\n",
       "13     1.949367\n",
       "10     0.405063\n",
       "20     0.392405\n",
       "6      0.215190\n",
       "16     0.088608\n",
       "12     0.075949\n",
       "40     0.063291\n",
       "21     0.037975\n",
       "15     0.025316\n",
       "7      0.025316\n",
       "32     0.012658\n",
       "34     0.012658\n",
       "14     0.012658\n",
       "24     0.012658\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifico balanceo de clases\n",
    "user_stories_train['storypoint'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538bcda7",
   "metadata": {},
   "source": [
    "### Preparamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "xBfg8oHRGKFN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBfg8oHRGKFN",
    "outputId": "af44b244-57d8-44b1-955b-6ca04803bf8b"
   },
   "outputs": [],
   "source": [
    "#Creamos un dataset con features a usar para clasificar\n",
    "user_stories_x = user_stories_train['description'].copy()\n",
    "\n",
    "#Creamos un dataset con la variable target 'storypoint'\n",
    "user_stories_y = user_stories_train['storypoint'].copy()\n",
    "\n",
    "#Genero los conjuntos de train y test\n",
    "x_train, x_test, y_train, y_test = train_test_split(user_stories_x,\n",
    "                                                   user_stories_y,\n",
    "                                                   test_size=0.3,  #proporcion 70/30\n",
    "                                                   random_state=2) #semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "JsWoAHk8GGcA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JsWoAHk8GGcA",
    "outputId": "6ac11d56-6f99-44fa-8d35-ea560a669922"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2646    The plugin will need to contribute the icon. W...\n",
       "2724    The timer: component is used to generate messa...\n",
       "5159    h5.Description:  In CLI we have the ability to...\n",
       "976     We need to refactor the way how we package and...\n",
       "2187    Standalone Admin currently has no shiny banner...\n",
       "                              ...                        \n",
       "3606    h5. Description:  When launching Studio with C...\n",
       "5704    When a .tss file contains certain grammars, Ti...\n",
       "6637    The data that is entering a broadcast stream c...\n",
       "2575    Provide the infrastructure for HTTP GET /compl...\n",
       "7336    Studio shows an Empty Preferences window when ...\n",
       "Name: description, Length: 5530, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Vee6iDIHF05c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vee6iDIHF05c",
    "outputId": "caba3b93-589e-4be4-b6c7-c4e4f7fedefe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2646    8\n",
       "2724    2\n",
       "5159    8\n",
       "976     2\n",
       "2187    3\n",
       "       ..\n",
       "3606    5\n",
       "5704    2\n",
       "6637    2\n",
       "2575    3\n",
       "7336    3\n",
       "Name: storypoint, Length: 5530, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1088e33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "gpKJEuTgF5E5",
    "outputId": "ebbe5343-62e4-4d34-b532-60138b42151c"
   },
   "source": [
    "## Ensamble con Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bSB7j8BLtRB",
   "metadata": {
    "id": "2bSB7j8BLtRB"
   },
   "source": [
    "###Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c26efa8c-1b2f-47ac-9925-8ced8968e624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/users/pablo.prieto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/pablo.prieto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/users/pablo.prieto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d57b61a-379d-4017-84d3-6aa584fcd71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatizationAndStopwords(text):\n",
    "    wordNetLemmatizer = WordNetLemmatizer()\n",
    "    stopwordSets = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    wordsFilter = [wordNetLemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stopwordSets and word.isalpha()]\n",
    "    if not wordsFilter:\n",
    "        return \"empty\"\n",
    "    return ' '.join(wordsFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03a12461-d10a-41ab-a64a-2b901008d492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.857821\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.885278\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.852218\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.866629\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.839220\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.845763\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=5, stacking__xgb_regressor__n_estimators=100, svd__n_components=200, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time=12.9min\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.884268\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.883583\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.901385\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.892908\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.890082\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.853390\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=5, stacking__xgb_regressor__n_estimators=100, svd__n_components=200, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time=25.1min\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.902803\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.910992\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.905906\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.933597\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.873128\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.890395\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=5, stacking__xgb_regressor__n_estimators=100, svd__n_components=200, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time=26.5min\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.875226\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.875954\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.892342\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.905623\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.860978\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.841243\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=5, stacking__xgb_regressor__n_estimators=100, svd__n_components=200, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time=27.4min\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.865958\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.888669\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.893755\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.855044\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.850805\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.841525\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=5, stacking__xgb_regressor__n_estimators=100, svd__n_components=200, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time=23.4min\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.857821\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.885278\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.852218\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.866629\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.839220\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.845763\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=5, stacking__xgb_regressor__n_estimators=200, svd__n_components=100, tfidf__max_features=2000, tfidf__ngram_range=(1, 1); total time= 6.4min\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.884268\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.883583\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.901385\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.892908\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.890082\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.853390\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=5, stacking__xgb_regressor__n_estimators=200, svd__n_components=100, tfidf__max_features=2000, tfidf__ngram_range=(1, 1); total time= 6.4min\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.902803\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.910992\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.905906\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.933597\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.873128\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.890395\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=5, stacking__xgb_regressor__n_estimators=200, svd__n_components=100, tfidf__max_features=2000, tfidf__ngram_range=(1, 1); total time= 6.5min\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.875226\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.875954\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.892342\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.905623\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.860978\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.841243\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=5, stacking__xgb_regressor__n_estimators=200, svd__n_components=100, tfidf__max_features=2000, tfidf__ngram_range=(1, 1); total time= 6.5min\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.865958\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.888669\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.893755\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.855044\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.850805\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.841525\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=5, stacking__xgb_regressor__n_estimators=200, svd__n_components=100, tfidf__max_features=2000, tfidf__ngram_range=(1, 1); total time= 6.7min\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.857821\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.885278\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.852218\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.866629\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.839220\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.845763\n",
      "[CV] END stacking__final_estimator__alpha=0.1, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=3, stacking__xgb_regressor__n_estimators=100, svd__n_components=100, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time= 6.5min\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.884268\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.883583\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.901385\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.892908\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.890082\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.853390\n",
      "[CV] END stacking__final_estimator__alpha=0.1, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=3, stacking__xgb_regressor__n_estimators=100, svd__n_components=100, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time= 6.3min\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.902803\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.910992\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.905906\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.933597\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.873128\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.890395\n",
      "[CV] END stacking__final_estimator__alpha=0.1, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=3, stacking__xgb_regressor__n_estimators=100, svd__n_components=100, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time= 6.3min\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.875226\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.875954\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.892342\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.905623\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.860978\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.841243\n",
      "[CV] END stacking__final_estimator__alpha=0.1, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=3, stacking__xgb_regressor__n_estimators=100, svd__n_components=100, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time= 6.5min\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.865958\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.888669\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.893755\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.855044\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.850805\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 3.841525\n",
      "[CV] END stacking__final_estimator__alpha=0.1, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=3, stacking__xgb_regressor__n_estimators=100, svd__n_components=100, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time= 6.8min\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.857821\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.885278\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.852218\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.866629\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.839220\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.845763\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=3, stacking__xgb_regressor__n_estimators=200, svd__n_components=200, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time=13.6min\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.884268\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.883583\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.901385\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.892908\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.890082\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.853390\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=3, stacking__xgb_regressor__n_estimators=200, svd__n_components=200, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time=13.6min\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.902803\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.910992\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.905906\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.933597\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3539, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.873128\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 3540, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.890395\n",
      "[CV] END stacking__final_estimator__alpha=1.0, stacking__lgbm_regressor__learning_rate=0.01, stacking__lgbm_regressor__num_leaves=50, stacking__rnd_regressor__max_depth=7, stacking__rnd_regressor__n_estimators=300, stacking__xgb_regressor__max_depth=3, stacking__xgb_regressor__n_estimators=200, svd__n_components=200, tfidf__max_features=1000, tfidf__ngram_range=(1, 1); total time=13.7min\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score 3.875226\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 115\u001b[0m\n\u001b[1;32m     99\u001b[0m randomcv_it \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(estimator \u001b[38;5;241m=\u001b[39m pipeline,\n\u001b[1;32m    100\u001b[0m                                  param_distributions \u001b[38;5;241m=\u001b[39m parameters,\n\u001b[1;32m    101\u001b[0m                                  scoring \u001b[38;5;241m=\u001b[39m scorer_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m                                  n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    106\u001b[0m                                  random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# #Grid Search Cross Validation\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# randomcv_it = GridSearchCV(estimator = pipeline,\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#                                  param_grid = parameters,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#Busco los hiperparamtros que optimizan F1 Score\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m \u001b[43mrandomcv_it\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m#Mejores hiperparametros del arbol\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfolds: \u001b[39m\u001b[38;5;124m\"\u001b[39m, folds)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1960\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1960\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 473\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/ensemble/_stacking.py:971\u001b[0m, in \u001b[0;36mStackingRegressor.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    969\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m    970\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 971\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/ensemble/_stacking.py:264\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[0;32m--> 264\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcross_val_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_method_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    282\u001b[0m     meth\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:1282\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m-> 1282\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1296\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:1367\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, fit_params, method)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1368\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(estimator, method)\n\u001b[1;32m   1369\u001b[0m predictions \u001b[38;5;241m=\u001b[39m func(X_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##KFOLD CV usando Ensamble con Stacking (los mejores atributos, hiperparametros,etc)\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#The Best: \n",
    "\n",
    "# parameters = {\n",
    "#     \"tfidf__analyzer\": [\"word\"],\n",
    "#     \"tfidf__ngram_range\": [(1, 1),(1, 2)],\n",
    "#     \"tfidf__use_idf\": [True,False],\n",
    "#     \"tfidf__stop_words\": [['english']],\n",
    "#     \"tfidf__max_features\": [1000, 2000],\n",
    "#     \"tfidf__min_df\": [2,5],\n",
    "#     \"tfidf__max_df\": [0.75, 0.8],\n",
    "#     \"tfidf__sublinear_tf\": [True,False],\n",
    "#     \"svd__n_components\": [100, 200, 300, 400, 500],\n",
    "#     \"svd__random_state\" :[42],\n",
    "#     'stacking__xgb_regressor__objective': ['reg:squarederror'],\n",
    "#     'stacking__xgb_regressor__random_state' :[42], \n",
    "#     'stacking__xgb_regressor__n_estimators': [100, 200, 300],\n",
    "#     'stacking__xgb_regressor__max_depth': [3, 4, 5],\n",
    "#     'stacking__xgb_regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "#     # 'stacking__xgb_regressor__subsample': [0.6, 0.7, 0.8, 0.9, 1],\n",
    "#     # 'stacking__xgb_regressor__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1],\n",
    "#     # 'stacking__xgb_regressor__gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "#     # 'stacking__xgb_regressor__reg_alpha': [0, 0.01, 0.1, 1, 10],\n",
    "#     # 'stacking__xgb_regressor__reg_lambda': [0, 0.01, 0.1, 1, 10]\n",
    "#     \"stacking__rnd_regressor__criterion\":[\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"],\n",
    "#     \"stacking__rnd_regressor__max_features\":['sqrt', 'log2'],\n",
    "#     \"stacking__rnd_regressor__max_depth\": [7],\n",
    "#     \"stacking__rnd_regressor__random_state\":[42], \n",
    "#     \"stacking__rnd_regressor__n_jobs\":[-1],\n",
    "#     \"stacking__rnd_regressor__min_samples_leaf\":[1, 2, 5],\n",
    "#     \"stacking__rnd_regressor__min_samples_split\":[2,5,10],\n",
    "#     \"stacking__rnd_regressor__n_estimators\":[300, 500],\n",
    "#     \"stacking__rnd_regressor__bootstrap\": [True],\n",
    "#     'stacking__lgbm_regressor__random_state' :[42], \n",
    "#     'stacking__lgbm_regressor__force_col_wise' :[True],\n",
    "#     'stacking__lgbm_regressor__n_estimators': [1000],\n",
    "#     'stacking__lgbm_regressor__learning_rate': [0.01],\n",
    "#     'stacking__lgbm_regressor__num_leaves': [50],\n",
    "#     'stacking__lgbm_regressor__max_depth': [10],\n",
    "#     'stacking__lgbm_regressor__min_child_samples': [10],\n",
    "#     'stacking__lgbm_regressor__min_child_weight': [0.1],\n",
    "#     # 'stacking__lgbm_regressor__min_data_in_leaf': [1],\n",
    "#     'stacking__lgbm_regressor__max_bin': [255],\n",
    "#     'stacking__lgbm_regressor__subsample': [0.8],\n",
    "#     'stacking__lgbm_regressor__colsample_bytree': [0.8],\n",
    "#     'stacking__lgbm_regressor__reg_lambda': [1.0],\n",
    "#     'stacking__lgbm_regressor__reg_alpha': [0.1],\n",
    "#     'stacking__final_estimator__random_state' :[42], \n",
    "#     'stacking__final_estimator__alpha': [0.1, 1.0, 10.0],\n",
    "#     'stacking__final_estimator__l1_ratio': [0.1, 0.5, 0.9],\n",
    "# }\n",
    "\n",
    "parameters = {\n",
    "    \"tfidf__max_features\": [1000, 2000],\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"svd__n_components\": [100, 200],\n",
    "    \"stacking__xgb_regressor__n_estimators\": [100, 200],\n",
    "    \"stacking__xgb_regressor__max_depth\": [3, 5],\n",
    "    \"stacking__rnd_regressor__n_estimators\": [300],\n",
    "    \"stacking__rnd_regressor__max_depth\": [7],\n",
    "    \"stacking__lgbm_regressor__num_leaves\": [50],\n",
    "    \"stacking__lgbm_regressor__learning_rate\": [0.01],\n",
    "    \"stacking__final_estimator__alpha\": [0.1, 1.0]\n",
    "}\n",
    "\n",
    "randomcv_best_score = None\n",
    "\n",
    "#Iteración Cantidad de splits para el Cross Validation\n",
    "# for folds in [9,10,12,15,18]:\n",
    "# for folds in [5,6,7,8,9,10]:\n",
    "# for folds in [4,5,10]:\n",
    "for folds in [5,10]:\n",
    "    \n",
    "    #Kfold\n",
    "    kfoldcv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    #Regressors\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(preprocessor=lemmatizationAndStopwords)),\n",
    "        ('svd', TruncatedSVD()),\n",
    "        ('stacking', StackingRegressor(\n",
    "            estimators=[\n",
    "                ('xgb_regressor', XGBRegressor()),\n",
    "                ('rnd_regressor', RandomForestRegressor()),\n",
    "                ('lgbm_regressor', LGBMRegressor(force_col_wise=True))\n",
    "            ],\n",
    "            final_estimator=ElasticNet()\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    #Metrica que quiero optimizar root_mean_squared_error\n",
    "    scorer_fn = make_scorer(root_mean_squared_error, greater_is_better=False)\n",
    "\n",
    "    #Random Search Cross Validation\n",
    "    #Cantidad de combinaciones que quiero probar\n",
    "    n=10\n",
    "    randomcv_it = RandomizedSearchCV(estimator = pipeline,\n",
    "                                     param_distributions = parameters,\n",
    "                                     scoring = scorer_fn,\n",
    "                                     cv = kfoldcv,\n",
    "                                     n_iter = n,\n",
    "                                     verbose = 2,\n",
    "                                     n_jobs = 1,\n",
    "                                     random_state = 42)\n",
    "    \n",
    "    # #Grid Search Cross Validation\n",
    "    # randomcv_it = GridSearchCV(estimator = pipeline,\n",
    "    #                                  param_grid = parameters,\n",
    "    #                                  scoring = scorer_fn,\n",
    "    #                                  cv = kfoldcv) \n",
    "\n",
    "    #Busco los hiperparamtros que optimizan F1 Score\n",
    "    randomcv_it.fit(x_train,y_train);\n",
    "    \n",
    "    #Mejores hiperparametros del arbol\n",
    "    print(\"folds: \", folds)\n",
    "    print(randomcv_it.best_params_)\n",
    "    #Mejor métrica\n",
    "    print(\"root_mean_squared_error: \", randomcv_it.best_score_)\n",
    "    print(\" \")\n",
    "    print(\"mean_test_score: \", randomcv_it.cv_results_['mean_test_score'])\n",
    "    \n",
    "    #Nos quedamos con el menor error root_mean_squared_error\n",
    "    if randomcv_best_score is None:\n",
    "        randomcv_best_score = randomcv_it\n",
    "    elif randomcv_it.best_score_ < randomcv_best_score.best_score_:\n",
    "        randomcv_best_score = randomcv_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7aaa06-4012-4b50-af1e-7fad3ee6894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomcv_best_score.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5730db3c-8708-46b0-a475-991ca3760ad4",
   "metadata": {},
   "source": [
    "THE BEST: 3.0575858141040166\n",
    "\n",
    "folds:  10\n",
    "{'tfidf__use_idf': True,\n",
    " 'tfidf__sublinear_tf': True,\n",
    " 'tfidf__stop_words': ['english'],\n",
    " 'tfidf__ngram_range': (1, 2),\n",
    " 'tfidf__min_df': 2,\n",
    " 'tfidf__max_features': 2000,\n",
    " 'tfidf__max_df': 0.75,\n",
    " 'tfidf__analyzer': 'word',\n",
    " 'svd__random_state': 42,\n",
    " 'svd__n_components': 400,\n",
    " 'stacking__xgb_regressor__random_state': 42,\n",
    " 'stacking__xgb_regressor__objective': 'reg:squarederror',\n",
    " 'stacking__xgb_regressor__n_estimators': 300,\n",
    " 'stacking__xgb_regressor__max_depth': 3,\n",
    " 'stacking__xgb_regressor__learning_rate': 0.01,\n",
    " 'stacking__rnd_regressor__random_state': 42,\n",
    " 'stacking__rnd_regressor__n_jobs': -1,\n",
    " 'stacking__rnd_regressor__n_estimators': 300,\n",
    " 'stacking__rnd_regressor__min_samples_split': 2,\n",
    " 'stacking__rnd_regressor__min_samples_leaf': 1,\n",
    " 'stacking__rnd_regressor__max_features': 'sqrt',\n",
    " 'stacking__rnd_regressor__max_depth': 7,\n",
    " 'stacking__rnd_regressor__criterion': 'squared_error',\n",
    " 'stacking__rnd_regressor__bootstrap': True,\n",
    " 'stacking__lgbm_regressor__subsample': 0.8,\n",
    " 'stacking__lgbm_regressor__random_state': 42,\n",
    " 'stacking__lgbm_regressor__num_leaves': 50,\n",
    " 'stacking__lgbm_regressor__n_estimators': 100,\n",
    " 'stacking__lgbm_regressor__min_child_samples': 5,\n",
    " 'stacking__lgbm_regressor__max_depth': 3,\n",
    " 'stacking__lgbm_regressor__learning_rate': 0.2,\n",
    " 'stacking__lgbm_regressor__colsample_bytree': 0.7,\n",
    " 'stacking__final_estimator__random_state': 42,\n",
    " 'stacking__final_estimator__l1_ratio': 0.5,\n",
    " 'stacking__final_estimator__alpha': 10.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9467ca-ed26-4b28-8f23-e48c9ddcc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomcv_best_score.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KK7j2I5cPOf4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KK7j2I5cPOf4",
    "outputId": "cedb38b5-48d1-4428-8ab9-6f32acbb0d4f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "randomcv_best_score.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc09200-e21a-49f8-aa56-b0d29192010f",
   "metadata": {},
   "source": [
    "THE BEST\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151eb22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
